{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53adda2e-9bd8-47f8-ba1d-e0d1447b6ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rites\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rites\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\rites\\AppData\\Local\\Temp\\ipykernel_14856\\4292542051.py:34: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  dictionary_df = dictionary_df.applymap(lambda x: preprocess(x) if isinstance(x, str) else x)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text-ID</th>\n",
       "      <th>Sentence-ID</th>\n",
       "      <th>Security</th>\n",
       "      <th>Conformity</th>\n",
       "      <th>Tradition</th>\n",
       "      <th>Benevolence</th>\n",
       "      <th>Universalism</th>\n",
       "      <th>Self-Direction</th>\n",
       "      <th>Stimulation</th>\n",
       "      <th>Hedonism</th>\n",
       "      <th>Achievement</th>\n",
       "      <th>Power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Text-ID  Sentence-ID  Security  Conformity  Tradition  Benevolence  \\\n",
       "0  BG_002            1         0           0          0            0   \n",
       "1  BG_002            2         0           0          0            0   \n",
       "2  BG_002            3         0           1          0            0   \n",
       "3  BG_002            4         0           0          0            0   \n",
       "4  BG_002            5         0           0          0            0   \n",
       "\n",
       "   Universalism  Self-Direction  Stimulation  Hedonism  Achievement  Power  \n",
       "0             0               0            0         0            0      0  \n",
       "1             0               0            0         0            0      0  \n",
       "2             0               0            0         0            0      2  \n",
       "3             0               0            0         0            0      0  \n",
       "4             2               1            0         1            0      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load dictionary and test data\n",
    "dictionary_path = 'Dictionary 10.xlsx'\n",
    "test_data_path = 'TEST DATA.csv'\n",
    "\n",
    "# Read dictionary from excel file\n",
    "dictionary_df = pd.read_excel(dictionary_path)\n",
    "\n",
    "# Read test data from csv file\n",
    "test_data_df = pd.read_csv(test_data_path)\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocess text: lowercase, remove stopwords, and lemmatize\n",
    "def preprocess(text):\n",
    "    return ' '.join(\n",
    "        lemmatizer.lemmatize(word)\n",
    "        for word in text.lower().split()\n",
    "        if word not in stop_words\n",
    "    )\n",
    "\n",
    "# Preprocess dictionary words\n",
    "dictionary_df = dictionary_df.applymap(lambda x: preprocess(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Preprocess test data sentences\n",
    "test_data_df['Text'] = test_data_df['Text'].astype(str).apply(preprocess)\n",
    "\n",
    "# Initialize output dataframe with the necessary columns\n",
    "output_df = test_data_df[['Text-ID', 'Sentence-ID']].copy()\n",
    "\n",
    "# Flatten the dictionary to create a unique vocabulary list\n",
    "vocabulary = dictionary_df.values.flatten()\n",
    "vocabulary = list(set([word for word in vocabulary if pd.notna(word)]))\n",
    "\n",
    "# Initialize TfidfVectorizer with the unique vocabulary\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocabulary, lowercase=False)\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = vectorizer.fit_transform(test_data_df['Text'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Merge the TF-IDF scores with the output DataFrame\n",
    "output_df = pd.concat([output_df, tfidf_df], axis=1)\n",
    "\n",
    "# Initialize dictionary count columns with zeros\n",
    "behavior_columns = [\"Security\", \"Conformity\", \"Tradition\", \"Benevolence\", \"Universalism\",\n",
    "                    \"Self-Direction\", \"Stimulation\", \"Hedonism\", \"Achievement\", \"Power\"]\n",
    "\n",
    "for column in behavior_columns:\n",
    "    output_df[column] = 0\n",
    "\n",
    "# Function to count matches\n",
    "def count_matches(sentence, words_list):\n",
    "    sentence_words = set(sentence.lower().split())\n",
    "    return sum(word in sentence_words for word in words_list)\n",
    "\n",
    "# Update output dataframe with counts\n",
    "for idx, row in test_data_df.iterrows():\n",
    "    sentence = row['Text']  # Assuming the sentence is in the 'Text' column\n",
    "    for column in behavior_columns:\n",
    "        words_list = dictionary_df[column].dropna().str.lower().tolist()\n",
    "        output_df.at[idx, column] = count_matches(sentence, words_list)\n",
    "\n",
    "# Select only the required columns for the output\n",
    "output_df = output_df[['Text-ID', 'Sentence-ID'] + behavior_columns]\n",
    "\n",
    "# Save the output dataframe to a new CSV file\n",
    "output_file_path = 'Filtered_Output.csv'\n",
    "output_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "output_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef1e1c74-6e63-41d0-a940-715c550602eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking and additional 21 columns added successfully and saved to final_ranked_output_with_behaviors_TFIDF.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the Excel file\n",
    "file_path = 'Filtered_Output.csv'  # Replace with the path to your file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# List of behavior columns to rank\n",
    "behavior_columns = ['Security', 'Conformity', 'Tradition', 'Benevolence', \n",
    "                    'Universalism', 'Self-Direction', 'Stimulation', \n",
    "                    'Hedonism', 'Achievement', 'Power']\n",
    "\n",
    "# Function to rank a row's values and assign rank to each behavior\n",
    "def rank_row(row):\n",
    "    # Replace 0s with pd.NA to exclude them from ranking\n",
    "    row_no_zero = row.replace(0, pd.NA)\n",
    "    # Rank the values, with the largest number being ranked 1, handle ties\n",
    "    ranks = row_no_zero.rank(method='min', ascending=False)\n",
    "    \n",
    "    # Prepare a dictionary for storing rank information\n",
    "    rank_dict = {}\n",
    "    \n",
    "    # Dictionary to store behaviors corresponding to each rank (Rank_1, Rank_2, ...)\n",
    "    rank_behavior_dict = {f'Rank_{i}': '' for i in range(1, 11)}\n",
    "    \n",
    "    # Loop over each behavior and assign ranks\n",
    "    for behavior in behavior_columns:\n",
    "        rank = ranks.get(behavior, None)\n",
    "        if pd.notna(rank):\n",
    "            # If the rank already has values, append this behavior\n",
    "            if rank_behavior_dict[f'Rank_{int(rank)}']:\n",
    "                rank_behavior_dict[f'Rank_{int(rank)}'] += ', ' + behavior\n",
    "            else:\n",
    "                rank_behavior_dict[f'Rank_{int(rank)}'] = behavior\n",
    "\n",
    "    # Add rank information for each behavior column\n",
    "    for col in behavior_columns:\n",
    "        rank_dict[f'{col}_Rank'] = ranks.get(col, ' ') if pd.notna(ranks.get(col)) else ' '\n",
    "\n",
    "    # Combine both dictionaries (rank behavior and rank columns)\n",
    "    return pd.Series({**rank_dict, **rank_behavior_dict})\n",
    "\n",
    "# Apply ranking function to each row for the behavior columns\n",
    "ranked_df = df[behavior_columns].apply(rank_row, axis=1)\n",
    "\n",
    "# Add columns for behavior rank and Rank_1 to Rank_10 values to the original dataframe\n",
    "df = pd.concat([df, ranked_df], axis=1)\n",
    "\n",
    "# Calculate the sum of the original behavior scores for each row\n",
    "df['Sum_of_Scores'] = df[behavior_columns].sum(axis=1)\n",
    "\n",
    "# Save the updated dataframe back to a new Excel file\n",
    "output_file = 'final_ranked_output_with_behaviors_TFIDF.xlsx'  # Specify the output file name\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Ranking and additional 21 columns added successfully and saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a5039cc-2ff7-4d19-b338-a522b5f4e768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert data successfully merged and saved to final_output_with_expert_analysis_TFIDF.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the previously generated file with ranks\n",
    "final_ranks_file = 'final_ranked_output_with_behaviors_TFIDF.xlsx'\n",
    "df_final = pd.read_excel(final_ranks_file)\n",
    "\n",
    "# Load the expert file (assume same structure as original data)\n",
    "expert_file = 'TEST SCORE UPD.csv'  # Update with actual expert file path\n",
    "df_expert = pd.read_csv(expert_file)\n",
    "\n",
    "# List of behavior columns\n",
    "behavior_columns = ['Security', 'Conformity', 'Tradition', 'Benevolence', \n",
    "                    'Universalism', 'Self-Direction', 'Stimulation', \n",
    "                    'Hedonism', 'Achievement', 'Power']\n",
    "\n",
    "# Merge the two dataframes on Text-ID and Sentence-ID\n",
    "df_merged = pd.merge(df_expert, df_final, on=['Text-ID', 'Sentence-ID'], suffixes=('_expert', '_final'))\n",
    "\n",
    "# Function to find human value from expert data and its corresponding rank\n",
    "def find_expert_value_and_rank(row):\n",
    "    # Find the human value based on the expert data (any column where value is greater than 0)\n",
    "    human_value = ', '.join([col for col in behavior_columns if row[f'{col}_expert'] > 0])\n",
    "    \n",
    "    # If no human value found, return blank and None for the rank\n",
    "    if not human_value:\n",
    "        return pd.Series({'Expert_Human_Value': '', 'Expert_Value_Rank': ''})\n",
    "    \n",
    "    # Find the rank corresponding to the human value(s) from the final ranks\n",
    "    rank_list = []\n",
    "    for value in human_value.split(', '):\n",
    "        rank = row[f'{value}_Rank']\n",
    "        rank_list.append(str(rank))\n",
    "    \n",
    "    # Join all ranks if multiple human values\n",
    "    rank_value = ', '.join(rank_list) if rank_list else ''\n",
    "    \n",
    "    return pd.Series({'Expert_Human_Value': human_value, 'Expert_Value_Rank': rank_value})\n",
    "\n",
    "# Apply the function to each row to get the expert human value and rank\n",
    "df_merged[['Expert_Human_Value', 'Expert_Value_Rank']] = df_merged.apply(find_expert_value_and_rank, axis=1)\n",
    "\n",
    "# Save the updated dataframe with the expert analysis columns\n",
    "output_file = 'final_output_with_expert_analysis_TFIDF.xlsx'\n",
    "df_merged.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Expert data successfully merged and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09fbfafd-d354-41c1-95ad-f8eff58a0a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text-ID</th>\n",
       "      <th>Sentence-ID</th>\n",
       "      <th>Security</th>\n",
       "      <th>Conformity</th>\n",
       "      <th>Tradition</th>\n",
       "      <th>Benevolence</th>\n",
       "      <th>Universalism</th>\n",
       "      <th>Self-Direction</th>\n",
       "      <th>Stimulation</th>\n",
       "      <th>Hedonism</th>\n",
       "      <th>Achievement</th>\n",
       "      <th>Power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Text-ID  Sentence-ID  Security  Conformity  Tradition  Benevolence  \\\n",
       "0  BG_002            1         0           0          0            0   \n",
       "1  BG_002            2         0           0          0            0   \n",
       "2  BG_002            3         0           0          0            0   \n",
       "3  BG_002            4         0           0          0            0   \n",
       "4  BG_002            5         0           0          0            0   \n",
       "\n",
       "   Universalism  Self-Direction  Stimulation  Hedonism  Achievement  Power  \n",
       "0             0               0            0         0            0      0  \n",
       "1             0               0            0         0            0      0  \n",
       "2             0               0            0         0            0      1  \n",
       "3             0               0            0         0            0      0  \n",
       "4             1               0            0         0            0      0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the previously generated output file\n",
    "output_file_path = 'Filtered_Output.csv'  # Update this path if necessary\n",
    "output_df = pd.read_csv(output_file_path)\n",
    "\n",
    "# Define the columns to process\n",
    "columns_to_process = [\"Self-Direction\", \"Stimulation\", \"Hedonism\", \"Achievement\", \"Power\", \"Security\",\n",
    "                      \"Tradition\", \"Conformity\", \"Benevolence\", \"Universalism\"]\n",
    "\n",
    "# Iterate over each row\n",
    "for idx, row in output_df.iterrows():\n",
    "    # Check if all values are 0\n",
    "    if (row[columns_to_process] == 0).all():\n",
    "        continue  # Skip updating this row if all values are 0\n",
    "    \n",
    "    # Find the maximum value for the row\n",
    "    max_value = row[columns_to_process].max()\n",
    "    \n",
    "    # Update columns: set to 1 if equal to max value, else 0\n",
    "    for column in columns_to_process:\n",
    "        output_df.at[idx, column] = 1 if row[column] == max_value else 0\n",
    "\n",
    "# Save the updated output dataframe to a new CSV file\n",
    "updated_output_file_path = 'TF_IDF_Output1.csv'  # Update this path if necessary\n",
    "output_df.to_csv(updated_output_file_path, index=False)\n",
    "\n",
    "output_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02024230-b6e3-41b2-b834-4dd8bf3cdb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score Columns:\n",
      "Index(['Text-ID', 'Sentence-ID', 'Security', 'Conformity', 'Tradition',\n",
      "       'Benevolence', 'Universalism', 'Self-Direction', 'Stimulation',\n",
      "       'Hedonism', 'Achievement', 'Power'],\n",
      "      dtype='object')\n",
      "\n",
      "Updated Output Columns:\n",
      "Index(['Text-ID', 'Sentence-ID', 'Security', 'Conformity', 'Tradition',\n",
      "       'Benevolence', 'Universalism', 'Self-Direction', 'Stimulation',\n",
      "       'Hedonism', 'Achievement', 'Power'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text-ID</th>\n",
       "      <th>Sentence-ID</th>\n",
       "      <th>Security_test</th>\n",
       "      <th>Conformity_test</th>\n",
       "      <th>Tradition_test</th>\n",
       "      <th>Benevolence_test</th>\n",
       "      <th>Universalism_test</th>\n",
       "      <th>Self-Direction_test</th>\n",
       "      <th>Stimulation_test</th>\n",
       "      <th>Hedonism_test</th>\n",
       "      <th>...</th>\n",
       "      <th>Tradition_updated</th>\n",
       "      <th>Benevolence_updated</th>\n",
       "      <th>Universalism_updated</th>\n",
       "      <th>Self-Direction_updated</th>\n",
       "      <th>Stimulation_updated</th>\n",
       "      <th>Hedonism_updated</th>\n",
       "      <th>Achievement_updated</th>\n",
       "      <th>Power_updated</th>\n",
       "      <th>Headers_1_test</th>\n",
       "      <th>Headers_1_updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Conformity</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Conformity</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Conformity,Hedonism</td>\n",
       "      <td>Power</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Conformity</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BG_002</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Conformity</td>\n",
       "      <td>Universalism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Text-ID  Sentence-ID  Security_test  Conformity_test  Tradition_test  \\\n",
       "0  BG_002            1              0                1               0   \n",
       "1  BG_002            2              0                1               0   \n",
       "2  BG_002            3              0                1               0   \n",
       "3  BG_002            4              0                1               0   \n",
       "4  BG_002            5              0                1               0   \n",
       "\n",
       "   Benevolence_test  Universalism_test  Self-Direction_test  Stimulation_test  \\\n",
       "0                 0                  0                    0                 0   \n",
       "1                 0                  0                    0                 0   \n",
       "2                 0                  0                    0                 0   \n",
       "3                 0                  0                    0                 0   \n",
       "4                 0                  0                    0                 0   \n",
       "\n",
       "   Hedonism_test  ...  Tradition_updated  Benevolence_updated  \\\n",
       "0              0  ...                  0                    0   \n",
       "1              0  ...                  0                    0   \n",
       "2              1  ...                  0                    0   \n",
       "3              0  ...                  0                    0   \n",
       "4              0  ...                  0                    0   \n",
       "\n",
       "   Universalism_updated  Self-Direction_updated  Stimulation_updated  \\\n",
       "0                     0                       0                    0   \n",
       "1                     0                       0                    0   \n",
       "2                     0                       0                    0   \n",
       "3                     0                       0                    0   \n",
       "4                     1                       0                    0   \n",
       "\n",
       "   Hedonism_updated  Achievement_updated  Power_updated       Headers_1_test  \\\n",
       "0                 0                    0              0           Conformity   \n",
       "1                 0                    0              0           Conformity   \n",
       "2                 0                    0              1  Conformity,Hedonism   \n",
       "3                 0                    0              0           Conformity   \n",
       "4                 0                    0              0           Conformity   \n",
       "\n",
       "   Headers_1_updated  \n",
       "0                     \n",
       "1                     \n",
       "2              Power  \n",
       "3                     \n",
       "4       Universalism  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the test score and updated output files\n",
    "test_score_path = 'TEST SCORE UPD.csv'  # Update this path if necessary\n",
    "updated_output_path = 'TF_IDF_Output1.csv'  # Update this path if necessary\n",
    "\n",
    "test_score_df = pd.read_csv(test_score_path)\n",
    "updated_output_df = pd.read_csv(updated_output_path)\n",
    "\n",
    "# Print the column names to check for discrepancies\n",
    "print(\"Test Score Columns:\")\n",
    "print(test_score_df.columns)\n",
    "\n",
    "print(\"\\nUpdated Output Columns:\")\n",
    "print(updated_output_df.columns)\n",
    "\n",
    "# Define the columns to process (update these if necessary based on the above output)\n",
    "columns_to_process = [\"Security\", \"Conformity\", \"Tradition\", \"Benevolence\", \"Universalism\", \n",
    "                      \"Self-Direction\", \"Stimulation\", \"Hedonism\", \"Achievement\", \"Power\"]\n",
    "\n",
    "# Merge dataframes based on Text-ID and Sentence-ID\n",
    "merged_df = test_score_df.merge(updated_output_df, on=['Text-ID', 'Sentence-ID'], suffixes=('_test', '_updated'))\n",
    "\n",
    "# Function to find headers with value 1\n",
    "def find_headers_with_one(row, suffix):\n",
    "    return ','.join([col for col in columns_to_process if row[f\"{col}{suffix}\"] == 1])\n",
    "\n",
    "# Add new columns for headers with value 1\n",
    "merged_df['Headers_1_test'] = merged_df.apply(lambda row: find_headers_with_one(row, '_test'), axis=1)\n",
    "merged_df['Headers_1_updated'] = merged_df.apply(lambda row: find_headers_with_one(row, '_updated'), axis=1)\n",
    "\n",
    "# Reorder columns\n",
    "final_columns = ['Text-ID', 'Sentence-ID'] + \\\n",
    "                [f\"{col}_test\" for col in columns_to_process] + \\\n",
    "                [f\"{col}_updated\" for col in columns_to_process] + \\\n",
    "                ['Headers_1_test', 'Headers_1_updated']\n",
    "\n",
    "final_df = merged_df[final_columns]\n",
    "\n",
    "# Save to a new Excel file\n",
    "output_excel_path = 'TF_IDF_Output1_combined.xlsx'  # Update this path if necessary\n",
    "final_df.to_excel(output_excel_path, index=False)\n",
    "\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7998829-b4ca-4900-9acd-0d031105d744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  Text-ID  Sentence-ID  Security_test  Conformity_test  Tradition_test  \\\n",
       " 0  BG_002          1.0            0.0              1.0             0.0   \n",
       " 1  BG_002          2.0            0.0              1.0             0.0   \n",
       " 2  BG_002          3.0            0.0              1.0             0.0   \n",
       " 3  BG_002          4.0            0.0              1.0             0.0   \n",
       " 4  BG_002          5.0            0.0              1.0             0.0   \n",
       " \n",
       "    Benevolence_test  Universalism_test  Self-Direction_test  Stimulation_test  \\\n",
       " 0               0.0                0.0                  0.0               0.0   \n",
       " 1               0.0                0.0                  0.0               0.0   \n",
       " 2               0.0                0.0                  0.0               0.0   \n",
       " 3               0.0                0.0                  0.0               0.0   \n",
       " 4               0.0                0.0                  0.0               0.0   \n",
       " \n",
       "    Hedonism_test  ...  Universalism_updated  Self-Direction_updated  \\\n",
       " 0            0.0  ...                   0.0                     0.0   \n",
       " 1            0.0  ...                   0.0                     0.0   \n",
       " 2            1.0  ...                   0.0                     0.0   \n",
       " 3            0.0  ...                   0.0                     0.0   \n",
       " 4            0.0  ...                   1.0                     0.0   \n",
       " \n",
       "    Stimulation_updated  Hedonism_updated  Achievement_updated  Power_updated  \\\n",
       " 0                  0.0               0.0                  0.0            0.0   \n",
       " 1                  0.0               0.0                  0.0            0.0   \n",
       " 2                  0.0               0.0                  0.0            1.0   \n",
       " 3                  0.0               0.0                  0.0            0.0   \n",
       " 4                  0.0               0.0                  0.0            0.0   \n",
       " \n",
       "         Headers_1_test  Headers_1_updated  Test_Match_Percentage  \\\n",
       " 0           Conformity                NaN                    0.0   \n",
       " 1           Conformity                NaN                    0.0   \n",
       " 2  Conformity,Hedonism              Power                    0.0   \n",
       " 3           Conformity                NaN                    0.0   \n",
       " 4           Conformity       Universalism                    0.0   \n",
       " \n",
       "    Updated_Match_Percentage  \n",
       " 0                       0.0  \n",
       " 1                       0.0  \n",
       " 2                       0.0  \n",
       " 3                       0.0  \n",
       " 4                       0.0  \n",
       " \n",
       " [5 rows x 26 columns],\n",
       " 0.37705295386231324,\n",
       " 0.3588391989157971)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the combined output file\n",
    "combined_output_path = 'TF_IDF_Output1_combined.xlsx'\n",
    "combined_df = pd.read_excel(combined_output_path)\n",
    "\n",
    "# Function to calculate the match percentage\n",
    "def calculate_match_percentage(test_headers, updated_headers):\n",
    "    if pd.isna(test_headers) and pd.isna(updated_headers):\n",
    "        return 1.0, 1.0\n",
    "    \n",
    "    test_words = set(str(test_headers).split(',')) if pd.notna(test_headers) else set()\n",
    "    updated_words = set(str(updated_headers).split(',')) if pd.notna(updated_headers) else set()\n",
    "    \n",
    "    matches = test_words.intersection(updated_words)\n",
    "    num_matches = len(matches)\n",
    "    \n",
    "    test_percentage = num_matches / len(test_words) if test_words else 0.0\n",
    "    updated_percentage = num_matches / len(updated_words) if updated_words else 0.0\n",
    "    \n",
    "    return test_percentage, updated_percentage\n",
    "\n",
    "# Apply the function to each row and create new columns\n",
    "combined_df[['Test_Match_Percentage', 'Updated_Match_Percentage']] = combined_df.apply(\n",
    "    lambda row: calculate_match_percentage(row['Headers_1_test'], row['Headers_1_updated']), axis=1, result_type='expand'\n",
    ")\n",
    "\n",
    "# Calculate the total score for each column\n",
    "total_rows = len(combined_df)\n",
    "test_total_score = combined_df['Test_Match_Percentage'].sum() / total_rows\n",
    "updated_total_score = combined_df['Updated_Match_Percentage'].sum() / total_rows\n",
    "\n",
    "# Add the total score to the dataframe\n",
    "combined_df.loc['Total'] = combined_df.sum(numeric_only=True)\n",
    "combined_df.at['Total', 'Test_Match_Percentage'] = test_total_score\n",
    "combined_df.at['Total', 'Updated_Match_Percentage'] = updated_total_score\n",
    "\n",
    "# Save the updated dataframe to a new Excel file\n",
    "updated_combined_output_path = 'TF_IDF_Output1_combined_final.xlsx'\n",
    "combined_df.to_excel(updated_combined_output_path, index=False)\n",
    "\n",
    "combined_df.head(), test_total_score, updated_total_score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
